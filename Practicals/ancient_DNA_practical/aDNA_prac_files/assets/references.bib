@article{souilmi.2024.pnas, 
year = {2024}, 
keywords = {aDNA,Conservation,Dingo}, 
title = {{Ancient genomes reveal over two thousand years of dingo population structure}}, 
author = {Souilmi, Yassine and Wasef, Sally and Williams, Matthew P and Conroy, Gabriel and Bar, Ido and Bover, Pere and Dann, Jackson and Heiniger, Holly and Llamas, Bastien and Ogbourne, Steven and Archer, Michael and Ballard, J William O and Reed, Elizabeth and Tobler, Raymond and Koungoulos, Loukas and Walshe, Keryn and Wright, Joanne L and Balme, Jane and O’Connor, Sue and Cooper, Alan and Mitchell, Kieren J}, 
journal = {Proceedings of the National Academy of Sciences}, 
issn = {0027-8424}, 
doi = {10.1073/pnas.2407584121}, 
pmid = {38976766}, 
abstract = {{Dingoes are culturally and ecologically important free-living canids whose ancestors arrived in Australia over 3,000 B.P., likely transported by seafaring people. However, the early history of dingoes in Australia—including the number of founding populations and their routes of introduction—remains uncertain. This uncertainty arises partly from the complex and poorly understood relationship between modern dingoes and New Guinea singing dogs, and suspicions that post-Colonial hybridization has introduced recent domestic dog ancestry into the genomes of many wild dingo populations. In this study, we analyzed genome-wide data from nine ancient dingo specimens ranging in age from 400 to 2,746 y old, predating the introduction of domestic dogs to Australia by European colonists. We uncovered evidence that the continent-wide population structure observed in modern dingo populations had already emerged several thousand years ago. We also detected excess allele sharing between New Guinea singing dogs and ancient dingoes from coastal New South Wales (NSW) compared to ancient dingoes from southern Australia, irrespective of any post-Colonial hybrid ancestry in the genomes of modern individuals. Our results are consistent with several demographic scenarios, including a scenario where the ancestry of dingoes from the east coast of Australia results from at least two waves of migration from source populations with varying affinities to New Guinea singing dogs. We also contribute to the growing body of evidence that modern dingoes derive little genomic ancestry from post-Colonial hybridization with other domestic dog lineages, instead descending primarily from ancient canids introduced to Sahul thousands of years ago.}}, 
pages = {e2407584121}, 
number = {30}, 
volume = {121}, 
month = {7}
}

@article{Zhang.2020.NatureCommunications, 
year = {2020}, 
title = {{Genomic regions under selection in the feralization of the dingoes}}, 
author = {Zhang, Shao-jie and Wang, Guo-Dong and Ma, Pengcheng and Zhang, Liang-liang and Yin, Ting-Ting and Liu, Yan-hu and Otecko, Newton O. and Wang, Meng and Ma, Ya-ping and Wang, Lu and Mao, Bingyu and Savolainen, Peter and Zhang, Ya-ping}, 
journal = {Nature Communications}, 
issn = {2041-1723}, 
doi = {10.1038/s41467-020-14515-6}, 
pmid = {32015346}, 
pmcid = {PMC6997406}, 
abstract = {{Dingoes are wild canids living in Australia, originating from domestic dogs. They have lived isolated from both the wild and the domestic ancestor, making them a unique model for studying feralization. Here, we sequence the genomes of 10 dingoes and 2 New Guinea Singing Dogs. Phylogenetic and demographic analyses show that dingoes originate from dogs in southern East Asia, which migrated via Island Southeast Asia to reach Australia around 8300 years ago, and subsequently diverged into a genetically distinct population. Selection analysis identifies 50 positively selected genes enriched in digestion and metabolism, indicating a diet change during feralization of dingoes. Thirteen of these genes have shifted allele frequencies compared to dogs but not compared to wolves. Functional assays show that an A-to-G mutation in ARHGEF7 decreases the endogenous expression, suggesting behavioral adaptations related to the transitions in environment. Our results indicate that the feralization of the dingo induced positive selection on genomic regions correlated to neurodevelopment, metabolism and reproduction, in adaptation to a wild environment. Dingoes evolved in isolation from both their domesticated and wild ancestors. Here, the authors investigate the genomic basis of the feralization of dingoes and trace their origin to domestic dogs that migrated to Australia approximately 8300 years ago.}}, 
pages = {671}, 
number = {1}, 
volume = {11}, 
language = {en}, 
keywords = {}
}

@article{Chen.2018.Bioinformatics, 
year = {2018}, 
title = {{fastp: an ultra-fast all-in-one FASTQ preprocessor}}, 
author = {Chen, Shifu and Zhou, Yanqing and Chen, Yaru and Gu, Jia}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/bty560}, 
pmid = {30423086}, 
pmcid = {PMC6129281}, 
abstract = {{Quality control and preprocessing of FASTQ files are essential to providing clean data for downstream analysis. Traditionally, a different tool is used for each operation, such as quality control, adapter trimming and quality filtering. These tools are often insufficiently fast as most are developed using high-level programming languages (e.g. Python and Java) and provide limited multi-threading support. Reading and loading data multiple times also renders preprocessing slow and I/O inefficient. We developed fastp as an ultra-fast FASTQ preprocessor with useful quality control and data-filtering features. It can perform quality control, adapter trimming, quality filtering, per-read quality pruning and many other operations with a single scan of the FASTQ data. This tool is developed in C++ and has multi-threading support. Based on our evaluation, fastp is 2–5 times faster than other FASTQ preprocessing tools such as Trimmomatic or Cutadapt despite performing far more operations than similar tools. The open-source code and corresponding instructions are available at https://github.com/OpenGene/fastp.}}, 
pages = {i884--i890}, 
number = {17}, 
volume = {34}, 
language = {en}, 
keywords = {}
}

@article{Ewels.2016.bioinformatics, 
year = {2016}, 
title = {{MultiQC: summarize analysis results for multiple tools and samples in a single report}}, 
author = {Ewels, Philip and Magnusson, Måns and Lundin, Sverker and Käller, Max}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/btw354}, 
pmid = {27312411}, 
pmcid = {PMC5039924}, 
abstract = {{Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis. Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization. Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.info Contact: phil.ewels@scilifelab.se}}, 
pages = {3047--3048}, 
number = {19}, 
volume = {32}, 
keywords = {}
}

@article{Lindgreen.2012.BMC, 
year = {2012}, 
title = {{AdapterRemoval: easy cleaning of next-generation sequencing reads}}, 
author = {Lindgreen, Stinus}, 
journal = {BMC Research Notes}, 
issn = {1756-0500}, 
doi = {10.1186/1756-0500-5-337}, 
pmid = {22748135}, 
pmcid = {PMC3532080}, 
abstract = {{With the advent of next-generation sequencing there is an increased demand for tools to pre-process and handle the vast amounts of data generated. One recurring problem is adapter contamination in the reads, i.e. the partial or complete sequencing of adapter sequences. These adapter sequences have to be removed as they can hinder correct mapping of the reads and influence SNP calling and other downstream analyses. We present a tool called AdapterRemoval which is able to pre-process both single and paired-end data. The program locates and removes adapter residues from the reads, it is able to combine paired reads if they overlap, and it can optionally trim low-quality nucleotides. Furthermore, it can look for adapter sequence in both the 5’ and 3’ ends of the reads. This is a flexible tool that can be tuned to accommodate different experimental settings and sequencing platforms producing FASTQ files. AdapterRemoval is shown to be good at trimming adapters from both single-end and paired-end data. AdapterRemoval is a comprehensive tool for analyzing next-generation sequencing data. It exhibits good performance both in terms of sensitivity and specificity. AdapterRemoval has already been used in various large projects and it is possible to extend it further to accommodate application-specific biases in the data.}}, 
pages = {337}, 
number = {1}, 
volume = {5}, 
language = {en}, 
keywords = {}
}

@article{ensembl, 
year = {2003}, 
month = {1}, 
title = {{Ensembl: a genome infrastructure.}}, 
author = {Birney, E and Team, Ensembl}, 
journal = {Cold Spring Harbor symposia on quantitative biology}, 
issn = {0091-7451}, 
doi = {10.1101/sqb.2003.68.213}, 
pmid = {15338620}, 
pages = {213--5}, 
number = {0}, 
volume = {68}
}

@article{Oliva.2021.Briefings, 
year = {2021}, 
title = {{Systematic benchmark of ancient DNA read mapping}}, 
author = {Oliva, Adrien and Tobler, Raymond and Cooper, Alan and Llamas, Bastien and Souilmi, Yassine}, 
journal = {Briefings in Bioinformatics}, 
issn = {1467-5463}, 
doi = {10.1093/bib/bbab076}, 
pmid = {33834210}, 
abstract = {{The current standard practice for assembling individual genomes involves mapping millions of short DNA sequences (also known as DNA ‘reads’) against a pre-constructed reference genome. Mapping vast amounts of short reads in a timely manner is a computationally challenging task that inevitably produces artefacts, including biases against alleles not found in the reference genome. This reference bias and other mapping artefacts are expected to be exacerbated in ancient DNA (aDNA) studies, which rely on the analysis of low quantities of damaged and very short DNA fragments (\textbackslashtextasciitilde30–80 bp). Nevertheless, the current gold-standard mapping strategies for aDNA studies have effectively remained unchanged for nearly a decade, during which time new software has emerged. In this study, we used simulated aDNA reads from three different human populations to benchmark the performance of 30 distinct mapping strategies implemented across four different read mapping software—BWA-aln, BWA-mem, NovoAlign and Bowtie2—and quantified the impact of reference bias in downstream population genetic analyses. We show that specific NovoAlign, BWA-aln and BWA-mem parameterizations achieve high mapping precision with low levels of reference bias, particularly after filtering out reads with low mapping qualities. However, unbiased NovoAlign results required the use of an IUPAC reference genome. While relevant only to aDNA projects where reference population data are available, the benefit of using an IUPAC reference demonstrates the value of incorporating population genetic information into the aDNA mapping process, echoing recent results based on graph genome representations.}}, 
pages = {bbab076}, 
number = {5}, 
volume = {22}, 
language = {en}, 
keywords = {}
}

@article{Jónsson.2013.Bioinformatics, 
year = {2013}, 
title = {{mapDamage2.0: fast approximate Bayesian estimates of ancient DNA damage parameters}}, 
author = {Jónsson, Hákon and Ginolhac, Aurélien and Schubert, Mikkel and Johnson, Philip L. F. and Orlando, Ludovic}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/btt193}, 
pmid = {23613487}, 
pmcid = {PMC3694634}, 
abstract = {{Motivation: Ancient DNA (aDNA) molecules in fossilized bones and teeth, coprolites, sediments, mummified specimens and museum collections represent fantastic sources of information for evolutionary biologists, revealing the agents of past epidemics and the dynamics of past populations. However, the analysis of aDNA generally faces two major issues. Firstly, sequences consist of a mixture of endogenous and various exogenous backgrounds, mostly microbial. Secondly, high nucleotide misincorporation rates can be observed as a result of severe post-mortem DNA damage. Such misincorporation patterns are instrumental to authenticate ancient sequences versus modern contaminants. We recently developed the user-friendly mapDamage package that identifies such patterns from next-generation sequencing (NGS) sequence datasets. The absence of formal statistical modeling of the DNA damage process, however, precluded rigorous quantitative comparisons across samples. Results: Here, we describe mapDamage 2.0 that extends the original features of mapDamage by incorporating a statistical model of DNA damage. Assuming that damage events depend only on sequencing position and post-mortem deamination, our Bayesian statistical framework provides estimates of four key features of aDNA molecules: the average length of overhangs (λ), nick frequency (ν) and cytosine deamination rates in both double-stranded regions () and overhangs (). Our model enables rescaling base quality scores according to their probability of being damaged. mapDamage 2.0 handles NGS datasets with ease and is compatible with a wide range of DNA library protocols. Availability: mapDamage 2.0 is available at ginolhac.github.io/mapDamage/ as a Python package and documentation is maintained at the Centre for GeoGenetics Web site (geogenetics.ku.dk/publications/mapdamage2.0/). Contact:jonsson.hakon@gmail.com Supplementary information:Supplementary data are available at Bioinformatics online.}}, 
pages = {1682--1684}, 
number = {13}, 
volume = {29}, 
language = {en}, 
keywords = {}
}

@article{igv.2011, 
year = {2011}, 
month = {1}, 
title = {{Integrative genomics viewer}}, 
author = {Robinson, James T and Thorvaldsdóttir, Helga and Winckler, Wendy and Guttman, Mitchell and Lander, Eric S and Getz, Gad and Mesirov, Jill P}, 
journal = {Nature Biotechnology}, 
issn = {1087-0156}, 
doi = {10.1038/nbt.1754}, 
pmid = {21221095}, 
pmcid = {PMC3346182}, 
pages = {24--26}, 
number = {1}, 
volume = {29}
}

@article{igv.2013, 
year = {2013}, 
month = {3}, 
title = {{Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration}}, 
author = {Thorvaldsdóttir, Helga and Robinson, James T. and Mesirov, Jill P.}, 
journal = {Briefings in Bioinformatics}, 
issn = {1467-5463}, 
doi = {10.1093/bib/bbs017}, 
pmid = {22517427}, 
pmcid = {PMC3603213}, 
abstract = {{Data visualization is an essential component of genomic data analysis. However, the size and diversity of the data sets produced by today’s sequencing and array-based profiling methods present major challenges to visualization tools. The Integrative Genomics Viewer (IGV) is a high-performance viewer that efficiently handles large heterogeneous data sets, while providing a smooth and intuitive user experience at all levels of genome resolution. A key characteristic of IGV is its focus on the integrative nature of genomic studies, with support for both array-based and next-generation sequencing data, and the integration of clinical and phenotypic data. Although IGV is often used to view genomic data from public sources, its primary emphasis is to support researchers who wish to visualize and explore their own data sets or those from colleagues. To that end, IGV supports flexible loading of local and remote data sets, and is optimized to provide high-performance data visualization and exploration on standard desktop systems. IGV is freely available for download from http://www.broadinstitute.org/igv, under a GNU LGPL open-source license.}}, 
pages = {178--192}, 
number = {2}, 
volume = {14}
}

@article{Schuenemann.pnas, 
year = {2011}, 
month = {9}, 
title = {{Targeted enrichment of ancient pathogens yielding the pPCP1 plasmid of Yersinia pestis from victims of the Black Death}}, 
author = {Schuenemann, Verena J. and Bos, Kirsten and DeWitte, Sharon and Schmedes, Sarah and Jamieson, Joslyn and Mittnik, Alissa and Forrest, Stephen and Coombes, Brian K. and Wood, James W. and Earn, David J. D. and White, William and Krause, Johannes and Poinar, Hendrik N.}, 
journal = {Proceedings of the National Academy of Sciences}, 
issn = {0027-8424}, 
doi = {10.1073/pnas.1105107108}, 
pmid = {21876176}, 
pmcid = {PMC3179067}, 
abstract = {{Although investigations of medieval plague victims have identified Yersinia pestis as the putative etiologic agent of the pandemic, methodological limitations have prevented large-scale genomic investigations to evaluate changes in the pathogen's virulence over time. We screened over 100 skeletal remains from Black Death victims of the East Smithfield mass burial site (1348–1350, London, England). Recent methods of DNA enrichment coupled with high-throughput DNA sequencing subsequently permitted reconstruction of ten full human mitochondrial genomes (16 kb each) and the full pPCP1 (9.6 kb) virulence-associated plasmid at high coverage. Comparisons of molecular damage profiles between endogenous human and Y. pestis DNA confirmed its authenticity as an ancient pathogen, thus representing the longest contiguous genomic sequence for an ancient pathogen to date. Comparison of our reconstructed plasmid against modern Y. pestis shows identity with several isolates matching the Medievalis biovar; however, our chromosomal sequences indicate the victims were infected with a Y. pestis variant that has not been previously reported. Our data reveal that the Black Death in medieval Europe was caused by a variant of Y. pestis that may no longer exist, and genetic data carried on its pPCP1 plasmid were not responsible for the purported epidemiological differences between ancient and modern forms of Y. pestis infections.}}, 
pages = {E746--E752}, 
number = {38}, 
volume = {108}
}

@article{mcan, 
year = {2023}, 
title = {{McAN: a novel computational algorithm and platform for constructing and visualizing haplotype networks}}, 
author = {Li, Lun and Xu, Bo and Tian, Dongmei and Wang, Anke and Zhu, Junwei and Li, Cuiping and Li, Na and Zhao, Wei and Shi, Leisheng and Xue, Yongbiao and Zhang, Zhang and Bao, Yiming and Zhao, Wenming and Song, Shuhui}, 
journal = {Briefings in Bioinformatics}, 
issn = {1467-5463}, 
doi = {10.1093/bib/bbad174}, 
pmid = {37170752}, 
pmcid = {PMC10199771}, 
abstract = {{Haplotype networks are graphs used to represent evolutionary relationships between a set of taxa and are characterized by intuitiveness in analyzing genealogical relationships of closely related genomes. We here propose a novel algorithm termed McAN that considers mutation spectrum history (mutations in ancestry haplotype should be contained in descendant haplotype), node size (corresponding to sample count for a given node) and sampling time when constructing haplotype network. We show that McAN is two orders of magnitude faster than state-of-the-art algorithms without losing accuracy, making it suitable for analysis of a large number of sequences. Based on our algorithm, we developed an online web server and offline tool for haplotype network construction, community lineage determination, and interactive network visualization. We demonstrate that McAN is highly suitable for analyzing and visualizing massive genomic data and is helpful to enhance the understanding of genome evolution. Availability: Source code is written in C/C++ and available at https://github.com/Theory-Lun/McAN and https://ngdc.cncb.ac.cn/biocode/tools/BT007301 under the MIT license. Web server is available at https://ngdc.cncb.ac.cn/bit/hapnet/. SARS-CoV-2 dataset are available at https://ngdc.cncb.ac.cn/ncov/. Contact: songshh@big.ac.cn (Song S), zhaowm@big.ac.cn (Zhao W), baoym@big.ac.cn (Bao Y), zhangzhang@big.ac.cn (Zhang Z), ybxue@big.ac.cn (Xue Y).}}, 
pages = {bbad174}, 
number = {3}, 
volume = {24}, 
keywords = {}
}

@article{Li.2009.Bioinformatics.bwa, 
year = {2009}, 
title = {{Fast and accurate short read alignment with Burrows–Wheeler transform}}, 
author = {Li, Heng and Durbin, Richard}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/btp324}, 
pmid = {19451168}, 
pmcid = {PMC2705234}, 
abstract = {{Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ∼10–20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. Availability: http://maq.sourceforge.net Contact: rd@sanger.ac.uk}}, 
pages = {1754--1760}, 
number = {14}, 
volume = {25}, 
language = {en}, 
note = {Li, Heng Durbin, Richard eng 077192/Z/05/Z/Wellcome Trust/United Kingdom Research Support, Non-U.S. Gov't England 2009/05/20 09:00 Bioinformatics. 2009 Jul 15;25(14):1754-60. doi: 10.1093/bioinformatics/btp324. Epub 2009 May 18.}, 
keywords = {}
}

@article{Li.2009.samtools, 
year = {2009}, 
title = {{The Sequence Alignment/Map format and SAMtools}}, 
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard and Subgroup, 1000 Genome Project Data Processing}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/btp352}, 
pmid = {19505943}, 
pmcid = {PMC2723002}, 
abstract = {{Summary: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. Availability: http://samtools.sourceforge.net Contact: rd@sanger.ac.uk}}, 
pages = {2078--2079}, 
number = {16}, 
volume = {25}, 
language = {en}, 
note = {Li, Heng Handsaker, Bob Wysoker, Alec Fennell, Tim Ruan, Jue Homer, Nils Marth, Gabor Abecasis, Goncalo Durbin, Richard eng 077192/Z/05/Z/Wellcome Trust/United Kingdom R01 HG004719/HG/NHGRI NIH HHS/ R01 HG004719-01/HG/NHGRI NIH HHS/ R01 HG004719-02/HG/NHGRI NIH HHS/ R01 HG004719-02S1/HG/NHGRI NIH HHS/ R01 HG004719-03/HG/NHGRI NIH HHS/ R01 HG004719-04/HG/NHGRI NIH HHS/ U54HG002750/HG/NHGRI NIH HHS/ Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't England 2009/06/10 09:00 Bioinformatics. 2009 Aug 15;25(16):2078-9. doi: 10.1093/bioinformatics/btp352. Epub 2009 Jun 8.}, 
keywords = {}
}

@article{Garcia.pone.haplonets, 
year = {2021}, 
title = {{Haplotype network branch diversity, a new metric combining genetic and topological diversity to compare the complexity of haplotype networks}}, 
author = {Garcia, Eric and Wright, Daniel and Gatins, Remy and Roberts, May B. and Pinheiro, Hudson T. and Salas, Eva and Chen, Jei-Ying and Winnikoff, Jacob R. and Bernardi, Giacomo}, 
journal = {PLoS ONE}, 
doi = {10.1371/journal.pone.0251878}, 
pmid = {34191803}, 
pmcid = {PMC8244886}, 
abstract = {{A common way of illustrating phylogeographic results is through the use of haplotype networks. While these networks help to visualize relationships between individuals, populations, and species, evolutionary studies often only quantitatively analyze genetic diversity among haplotypes and ignore other network properties. Here, we present a new metric, haplotype network branch diversity (HBd), as an easy way to quantifiably compare haplotype network complexity. Our metric builds off the logic of combining genetic and topological diversity to estimate complexity previously used by the published metric haplotype network diversity (HNd). However, unlike HNd which uses a combination of network features to produce complexity values that cannot be defined in probabilistic terms, thereby obscuring the values’ implication for a sampled population, HBd uses frequencies of haplotype classes to incorporate topological information of networks, keeping the focus on the population and providing easy-to-interpret probabilistic values for randomly sampled individuals. The goal of this study is to introduce this more intuitive metric and provide an R script that allows researchers to calculate diversity and complexity indices from haplotype networks. A group of datasets, generated manually (model dataset) and based on published data (empirical dataset), were used to illustrate the behavior of HBd and both of its terms, haplotype diversity, and a new index called branch diversity. Results followed a predicted trend in both model and empirical datasets, from low metric values in simple networks to high values in complex networks. In short, the new combined metric joins genetic and topological diversity of haplotype networks, into a single complexity value. Based on our analysis, we recommend the use of HBd, as it makes direct comparisons of network complexity straightforward and provides probabilistic values that can readily discriminate situations that are difficult to resolve with available metrics.}}, 
pages = {e0251878}, 
number = {6}, 
volume = {16}, 
keywords = {}
}

@article{Danecek.2021.GigaScience, 
year = {2021}, 
title = {{Twelve years of SAMtools and BCFtools}}, 
author = {Danecek, Petr and Bonfield, James K and Liddle, Jennifer and Marshall, John and Ohan, Valeriu and Pollard, Martin O and Whitwham, Andrew and Keane, Thomas and McCarthy, Shane A and Davies, Robert M and Li, Heng}, 
journal = {GigaScience}, 
issn = {2047-217X}, 
doi = {10.1093/gigascience/giab008}, 
pmid = {33590861}, 
pmcid = {PMC7931819}, 
abstract = {{SAMtools and BCFtools are widely used programs for processing and analysing high-throughput sequencing data. They include tools for file format conversion and manipulation, sorting, querying, statistics, variant calling, and effect analysis amongst other methods. The first version appeared online 12 years ago and has been maintained and further developed ever since, with many new features and improvements added over the years. The SAMtools and BCFtools packages represent a unique collection of tools that have been used in numerous other software projects and countless genomic pipelines. Both SAMtools and BCFtools are freely available on GitHub under the permissive MIT licence, free for both non-commercial and commercial use. Both packages have been installed >1 million times via Bioconda. The source code and documentation are available from https://www.htslib.org.}}, 
pages = {giab008}, 
number = {2}, 
volume = {10}, 
language = {en}, 
keywords = {}
}

@article{Danecek.2011.Bioinformatics, 
year = {2011}, 
title = {{The variant call format and VCFtools}}, 
author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard and Group, 1000 Genomes Project Analysis}, 
journal = {Bioinformatics}, 
issn = {1367-4803}, 
doi = {10.1093/bioinformatics/btr330}, 
pmid = {21653522}, 
pmcid = {PMC3137218}, 
abstract = {{Summary: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API. Availability: http://vcftools.sourceforge.net Contact: rd@sanger.ac.uk}}, 
pages = {2156--2158}, 
number = {15}, 
volume = {27}, 
language = {en}, 
note = {Danecek, Petr Auton, Adam Abecasis, Goncalo Albers, Cornelis A Banks, Eric DePristo, Mark A Handsaker, Robert E Lunter, Gerton Marth, Gabor T Sherry, Stephen T McVean, Gilean Durbin, Richard eng 075491/Z/04/Wellcome Trust/United Kingdom 086084/Wellcome Trust/United Kingdom 090532/Wellcome Trust/United Kingdom 54 HG003067/HG/NHGRI NIH HHS/ R01 HG004719/HG/NHGRI NIH HHS/ RG/09/012/28096/British Heart Foundation/United Kingdom RG/09/012/28096/Wellcome Trust/United Kingdom T32 HG000044/HG/NHGRI NIH HHS/ U01 HG005208/HG/NHGRI NIH HHS/ Research Support, N.I.H., Extramural Research Support, N.I.H., Intramural Research Support, Non-U.S. Gov't England Oxford, England 2011/06/10 06:00 Bioinformatics. 2011 Aug 1;27(15):2156-8. doi: 10.1093/bioinformatics/btr330. Epub 2011 Jun 7.}, 
keywords = {}
}

@misc{andrews2012,
  address = {{Babraham, UK}},
  title = {{{FastQC}}},
  copyright = {GPL v3},
  abstract = {FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.},
  howpublished = {Babraham Institute},
  author = {Andrews, Simon and Krueger, Felix and {Segonds-Pichon}, Anne and Biggins, Laura and Krueger, Christel and Wingett, Steven},
  month = jan,
  year = {2012}
}

@misc{Picard2018toolkit,
  author = {{Broad Institute}},
  title = {Picard Tools},
  journal = {{ Broad Institute, GitHub repository}},
  howpublished = {\url{http://broadinstitute.github.io/picard/}}
}

@article{jun_efficient_2015,
	title = {An efficient and scalable analysis framework for variant extraction and refinement from population scale {DNA} sequence data},
	issn = {1088-9051, 1549-5469},
	url = {https://genome.cshlp.org/content/early/2015/04/14/gr.176552.114},
	doi = {10.1101/gr.176552.114},
	abstract = {The analysis of next-generation sequencing data is computationally and statistically challenging because of massive data volumes and imperfect data quality. We present GotCloud, a pipeline for efficiently detecting and genotyping high-quality variants from large-scale sequencing data. GotCloud automates sequence alignment, sample-level quality control, variant calling, filtering of likely artifacts using machine learning techniques, and genotype refinement using haplotype information. The pipeline can process thousands of samples in parallel and requires less computational resources than current alternatives. Experiments with whole genome and exome targeted sequence data generated by the 1000 Genomes Project show that the pipeline provides effective filtering against false positive variants and high power to detect true variants. Our pipeline has already contributed to variant detection and genotyping in several large-scale sequencing projects, including the 1000 Genomes Project and the NHLBI Exome Sequencing Project. We hope it will now prove useful to many medical sequencing studies.},
	language = {en},
	urldate = {2024-10-02},
	journal = {Genome Research},
	author = {Jun, Goo and Wing, Mary Kate and Abecasis, Gonçalo R. and Kang, Hyun Min},
	month = apr,
	year = {2015},
	pmid = {25883319},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	keywords = {DNA sequencing, sequence analysis, SNP calling, variant calling, variant filtering},
	pages = {gr.176552.114},
}

@manual{seqmagick,
    title = {seqmagick: Sequence Manipulation Utilities},
    author = {Guangchuang Yu},
    year = {2024},
    note = {R package version 0.1.7},
    url = {https://github.com/yulab-smu/seqmagick},
}

@article{edgar_muscle_2004,
	title = {{MUSCLE}: multiple sequence alignment with high accuracy and high throughput},
	volume = {32},
	issn = {0305-1048},
	shorttitle = {{MUSCLE}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC390337/},
	doi = {10.1093/nar/gkh340},
	abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5.com/muscle.},
	number = {5},
	urldate = {2024-10-02},
	journal = {Nucleic Acids Research},
	author = {Edgar, Robert C.},
	year = {2004},
	pmid = {15034147},
	pmcid = {PMC390337},
	pages = {1792--1797},
}

@article{leigh_popart_2015,
	title = {popart: full-feature software for haplotype network construction},
	volume = {6},
	copyright = {© 2015 The Authors. Methods in Ecology and Evolution © 2015 British Ecological Society},
	issn = {2041-210X},
	shorttitle = {popart},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12410},
	doi = {10.1111/2041-210X.12410},
	abstract = {Haplotype networks are an intuitive method for visualising relationships between individual genotypes at the population level. Here, we present popart, an integrated software package that provides a comprehensive implementation of haplotype network methods, phylogeographic visualisation tools and standard statistical tests, together with publication-ready figure production. popart also provides a platform for the implementation and distribution of new network-based methods – we describe one such new method, integer neighbour-joining. The software is open source and freely available for all major operating systems.},
	language = {en},
	number = {9},
	urldate = {2024-10-02},
	journal = {Methods in Ecology and Evolution},
	author = {Leigh, Jessica W. and Bryant, David},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12410},
	keywords = {AMOVA, haplotype networks, phylogeography, polynesian expansion, population genetics, TCS},
	pages = {1110--1116},
}

@article{bandelt_median-joining_1999,
	title = {Median-joining networks for inferring intraspecific phylogenies},
	volume = {16},
	issn = {0737-4038},
	doi = {10.1093/oxfordjournals.molbev.a026036},
	abstract = {Reconstructing phylogenies from intraspecific data (such as human mitochondrial DNA variation) is often a challenging task because of large sample sizes and small genetic distances between individuals. The resulting multitude of plausible trees is best expressed by a network which displays alternative potential evolutionary paths in the form of cycles. We present a method ("median joining" [MJ]) for constructing networks from recombination-free population data that combines features of Kruskal's algorithm for finding minimum spanning trees by favoring short connections, and Farris's maximum-parsimony (MP) heuristic algorithm, which sequentially adds new vertices called "median vectors", except that our MJ method does not resolve ties. The MJ method is hence closely related to the earlier approach of Foulds, Hendy, and Penny for estimating MP trees but can be adjusted to the level of homoplasy by setting a parameter epsilon. Unlike our earlier reduced median (RM) network method, MJ is applicable to multistate characters (e.g., amino acid sequences). An additional feature is the speed of the implemented algorithm: a sample of 800 worldwide mtDNA hypervariable segment I sequences requires less than 3 h on a Pentium 120 PC. The MJ method is demonstrated on a Tibetan mitochondrial DNA RFLP data set.},
	language = {eng},
	number = {1},
	journal = {Molecular Biology and Evolution},
	author = {Bandelt, H. J. and Forster, P. and Röhl, A.},
	month = jan,
	year = {1999},
	pmid = {10331250},
	keywords = {Algorithms, DNA, Mitochondrial, Genome, Human, Humans, Models, Genetic, Phylogeny},
	pages = {37--48},
}

@article{mafft,
    author = {Katoh, Kazutaka and Misawa, Kazuharu and Kuma, Kei‐ichi and Miyata, Takashi},
    title = "{MAFFT: a novel method for rapid multiple sequence alignment based on fast Fourier transform}",
    journal = {Nucleic Acids Research},
    volume = {30},
    number = {14},
    pages = {3059-3066},
    year = {2002},
    month = {07},
    abstract = "{A multiple sequence alignment program, MAFFT, has been developed. The CPU time is drastically reduced as compared with existing methods. MAFFT includes two novel techniques. (i) Homo logous regions are rapidly identified by the fast Fourier transform (FFT), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) We propose a simplified scoring system that performs well for reducing CPU time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. Two different heuristics, the progressive method (FFT‐NS‐2) and the iterative refinement method (FFT‐NS‐i), are implemented in MAFFT. The performances of FFT‐NS‐2 and FFT‐NS‐i were compared with other methods by computer simulations and benchmark tests; the CPU time of FFT‐NS‐2 is drastically reduced as compared with CLUSTALW with comparable accuracy. FFT‐NS‐i is over 100 times faster than T‐COFFEE, when the number of input sequences exceeds 60, without sacrificing the accuracy.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gkf436},
    url = {https://doi.org/10.1093/nar/gkf436},
    eprint = {https://academic.oup.com/nar/article-pdf/30/14/3059/9488148/gkf436.pdf},
}